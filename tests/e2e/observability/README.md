# Arshai Observability End-to-End Test Suite

This directory contains a comprehensive end-to-end test suite for the Arshai observability system. It verifies that all 4 key metrics are properly collected, exported to real observability backends, and that there are no side effects on LLM calls.

## ğŸ¯ What This Tests

### Key Metrics (As Requested)
- âœ… **`llm_time_to_first_token_seconds`** - Time from request start to first token
- âœ… **`llm_time_to_last_token_seconds`** - Time from request start to last token  
- âœ… **`llm_duration_first_to_last_token_seconds`** - Duration from first token to last token
- âœ… **`llm_completion_tokens`** - Count of completion tokens generated

### Full Stack Integration
- **OpenTelemetry Collector**: Receives and processes telemetry data
- **Jaeger**: Distributed tracing backend
- **Prometheus**: Metrics collection and storage
- **Grafana**: Visualization dashboards

### LLM Provider Testing
- **OpenAI**: Complete observability integration
- **Azure OpenAI**: Token counting and metrics (if API key provided)
- **Anthropic Claude**: Native token counting (if API key provided)
- **Google Gemini**: Token estimation (if API key provided)

### Test Scenarios
- **Simple Chat Completion**: Basic request/response with timing
- **Streaming Completion**: Real-time token timing and chunk processing
- **Concurrent Requests**: Multiple parallel requests with individual tracking
- **Error Handling**: Graceful degradation when observability fails

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Arshai LLM    â”‚â”€â”€â”€â–¶â”‚ OTLP Collector  â”‚â”€â”€â”€â–¶â”‚    Backends     â”‚
â”‚  + Observabilityâ”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â”‚                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                              â”‚                   â”‚ Jaeger  â”‚
                              â”‚                   â”‚(Traces) â”‚
                              â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â”‚                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Prometheusâ”‚
                                                  â”‚(Metrics)â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                                                  â”‚ Grafana â”‚
                                                  â”‚(Dashboards)â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

1. **Docker & Docker Compose**
   ```bash
   # Check if Docker is running
   docker info
   
   # Install if needed
   # macOS: brew install docker docker-compose
   # Ubuntu: apt install docker.io docker-compose
   ```

2. **Python 3.8+**
   ```bash
   python3 --version
   ```

3. **OpenAI API Key** (required)
   ```bash
   export OPENAI_API_KEY="your-openai-api-key"
   ```

4. **Optional API Keys** (for additional provider testing)
   ```bash
   export ANTHROPIC_API_KEY="your-anthropic-key"
   export GOOGLE_API_KEY="your-google-key"
   export AZURE_OPENAI_API_KEY="your-azure-key"
   ```

### Running the Test

```bash
# Navigate to test directory
cd tests/e2e/observability/

# Run the complete test suite
./run_test.sh
```

The script will:
1. âœ… Start all observability services (Docker Compose)
2. âœ… Wait for services to be ready
3. âœ… Install Python dependencies
4. âœ… Run comprehensive tests
5. âœ… Verify metrics and traces collection
6. âœ… Provide access URLs for manual inspection

## ğŸ“Š Expected Output

### Test Console Output
```
ğŸš€ Arshai Observability End-to-End Test Suite
==============================================================

ğŸ“‹ PHASE 1: DEPENDENCY CHECKS
âœ… Dependencies
âœ… OTLP Collector
âœ… Jaeger  
âœ… Prometheus

ğŸ“‹ PHASE 2: PROVIDER TESTS
ğŸ§ª Testing openai provider with observability...
  ğŸ“ Testing simple chat completion...
  âœ… Simple completion: 45 tokens in 1.23s
  ğŸ“ Testing streaming completion...
  âœ… Streaming: 8 chunks, 52 tokens
  ğŸ“ Testing concurrent requests...
  âœ… Concurrent: 3/3 successful in 2.15s

ğŸ“‹ PHASE 3: OBSERVABILITY VERIFICATION
ğŸ” Verifying metrics collection...
  âœ… llm_time_to_first_token_seconds: 3 series
  âœ… llm_time_to_last_token_seconds: 3 series
  âœ… llm_duration_first_to_last_token_seconds: 3 series
  âœ… llm_completion_tokens: 3 series
âœ… Metrics verification: 4/4 key metrics found

ğŸ” Verifying traces collection...
âœ… Found 9 traces in Jaeger
  ğŸ“Š Found 15 LLM-related spans

ğŸ“‹ PHASE 4: TEST SUMMARY
============================================================
ğŸ“Š END-TO-END TEST SUMMARY
============================================================
Tests Run: 8
Tests Passed: 8
Tests Failed: 0
Success Rate: 100.0%
Providers Tested: openai

ğŸ¯ KEY METRICS STATUS:
  âœ… llm_time_to_first_token_seconds: 3 series
  âœ… llm_time_to_last_token_seconds: 3 series
  âœ… llm_duration_first_to_last_token_seconds: 3 series
  âœ… llm_completion_tokens: 3 series

ğŸ’¡ NEXT STEPS:
  1. View metrics: http://localhost:9090 (Prometheus)
  2. View traces: http://localhost:16686 (Jaeger)
  3. View dashboards: http://localhost:3000 (Grafana, admin/admin)
============================================================

ğŸ‰ END-TO-END TEST SUITE: PASSED
```

## ğŸ” Manual Verification

After tests complete, you can manually verify the observability data:

### 1. Jaeger (Traces) - http://localhost:16686
- **Service**: `arshai-e2e-test`
- **Operations**: Look for `llm.chat_completion`, `llm.stream_completion`
- **Spans**: Each span should have timing attributes and token counts
- **Search**: Try searching by service or operation name

### 2. Prometheus (Metrics) - http://localhost:9090
Query these key metrics:
```promql
# Key metrics
llm_time_to_first_token_seconds
llm_time_to_last_token_seconds
llm_duration_first_to_last_token_seconds
llm_completion_tokens

# Additional metrics
llm_requests_total
llm_active_requests
llm_request_duration_seconds
```

### 3. Grafana (Dashboards) - http://localhost:3000
- **Login**: admin/admin
- **Dashboard**: "Arshai LLM Observability"
- **Panels**: Pre-configured panels for all key metrics
- **Time Range**: Last 30 minutes (adjust as needed)

## ğŸ› ï¸ Manual Testing

You can also run individual components manually:

### Start Only Infrastructure
```bash
docker-compose up -d
```

### Run Only Python Tests
```bash
# Install dependencies
pip install -r requirements.txt

# Set API key
export OPENAI_API_KEY="your-key"

# Run tests
python test_e2e_observability.py
```

### Check Service Status
```bash
# Check all services
docker-compose ps

# Check logs
docker-compose logs otel-collector
docker-compose logs jaeger
docker-compose logs prometheus
docker-compose logs grafana
```

## ğŸ› Troubleshooting

### Common Issues

#### 1. Services Not Starting
```bash
# Check Docker is running
docker info

# Check ports are available
lsof -i :4317  # OTLP Collector
lsof -i :16686 # Jaeger
lsof -i :9090  # Prometheus
lsof -i :3000  # Grafana

# Restart services
docker-compose down
docker-compose up -d
```

#### 2. No Metrics in Prometheus
```bash
# Check OTLP Collector logs
docker-compose logs otel-collector

# Check if metrics endpoint is accessible
curl http://localhost:8889/metrics

# Verify Prometheus config
curl http://localhost:9090/api/v1/status/config
```

#### 3. No Traces in Jaeger
```bash
# Check Jaeger logs
docker-compose logs jaeger

# Check if Jaeger API is working
curl http://localhost:16686/api/services

# Verify OTLP export is working
curl http://localhost:4317  # Should connect
```

#### 4. Test Failures
```bash
# Run with debug logging
export PYTHONPATH=/path/to/arshai
python -c "
import logging
logging.basicConfig(level=logging.DEBUG)
import asyncio
from test_e2e_observability import main
asyncio.run(main())
"

# Check API keys are set
echo $OPENAI_API_KEY
```

### Debug Mode

Enable detailed logging by modifying the test:

```python
# In test_e2e_observability.py, change logging level
logging.basicConfig(
    level=logging.DEBUG,  # Changed from INFO
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

## ğŸ“ File Structure

```
tests/e2e/observability/
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ docker-compose.yml            # Full observability stack
â”œâ”€â”€ otel-collector-config.yaml    # OTLP Collector configuration
â”œâ”€â”€ prometheus.yml                # Prometheus configuration
â”œâ”€â”€ grafana-datasources.yml       # Grafana data sources
â”œâ”€â”€ grafana-dashboards.yml        # Grafana dashboard config
â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ arshai-llm-metrics.json   # Pre-built LLM dashboard
â”œâ”€â”€ test_config.yaml              # Test configuration
â”œâ”€â”€ test_e2e_observability.py     # Main test script
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ run_test.sh                   # Test runner script
```

## ğŸ”§ Configuration

### Modifying Test Configuration

Edit `test_config.yaml` to customize:

```yaml
observability:
  # Change service name
  service_name: "my-custom-test"
  
  # Adjust export intervals for testing
  metric_export_interval: 5  # Faster for testing
  
  # Enable/disable providers
  provider_configs:
    openai:
      track_token_timing: true
    anthropic:
      track_token_timing: false  # Disable if no API key
```

### Custom Docker Compose

Modify `docker-compose.yml` to:
- Change port mappings
- Add custom environment variables
- Use different image versions
- Add additional services

## ğŸ¯ Success Criteria

The test suite passes when:

1. **âœ… All 4 key metrics are collected** and visible in Prometheus
2. **âœ… Traces are exported** to Jaeger with proper span attributes
3. **âœ… LLM calls work normally** without side effects
4. **âœ… Multiple providers tested** (if API keys available)
5. **âœ… Streaming scenarios work** with token-level timing
6. **âœ… Concurrent requests** are individually tracked
7. **âœ… Error scenarios** are handled gracefully

## ğŸš€ Next Steps

After successful testing:

1. **Production Setup**: Use the Docker Compose as a template for production
2. **Custom Dashboards**: Build custom Grafana dashboards for your use case
3. **Alerting**: Set up Prometheus alerts for key metrics
4. **Integration**: Integrate with your existing observability infrastructure
5. **Scaling**: Configure for high-throughput production workloads

## ğŸ¤ Contributing

To extend the test suite:

1. **Add Providers**: Implement tests for additional LLM providers
2. **Add Metrics**: Test additional custom metrics
3. **Add Scenarios**: Test complex workflow scenarios
4. **Add Assertions**: Add more detailed validation logic
5. **Add Backends**: Test with other observability backends

The test suite is designed to be comprehensive yet extensible for your specific observability needs.